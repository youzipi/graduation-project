2  使用Scrapy框架实现爬虫Scrapy是一个用Python语言实现的爬虫框架[7]，任何人都可以根据各自的需求对其进行修改，然后实现对目标网站的抓取。
Scrapy框架的数据流向首先从初始URL开始。调度程序(Scheduler)接受请求，并对URL进行排序，接着发送请求给下载器（Downloader），下载器抓取网页并将网页内容返回给爬虫（Spiders）。爬虫分析出来的结果有两种：一种是需要进一步抓取的链接，放回调度器；另一种是需要保存的数据，则被送回项目管道（Item Pipeline），项目管道负责清洗数据，验证解析出来的数据，检查数据是否和已存储的数据重复，有则丢弃，最终将清洗后的数据存储到数据库中。


scrapy.cfg是整个项目的配置文件。items.py是爬
虫项目设置存储对象的文件。p加elines.py作用是将网页解
析后的对象存储到数据库。settings.py是项目的设置文件，
一是根据项目需要，设置爬虫的深度，避免爬虫在网页深
度上无限制抓取网页:二是设置爬虫的频率，爬虫抓取网
页的频率过快，会被该网站视作网络攻击被屏蔽。spiders
文件夹下放置的是实现爬虫逻辑的文件。


随着互联网的进一步快速发展，己有的搜索引擎所提供的搜索服务质量，跟不
上信息的爆炸式增长，逐渐不能满足用户的需求。Google于1998年成立，以
PageRank}S-}}链接分析等新技术大幅度提高了搜索质量，之后高速发展并抢占了绝大
多数搜索引擎市场，成为目前最重要的搜索引擎fgl。百度搜索引擎则依靠本地化的优
势，成为中国国内最强大的搜索引擎。国内市场除了百度，还出现了搜狗、360搜索
等其他搜索引擎。
尽管不论国际还是国内出现了一种新的现象，即成功的新互联网公司屏蔽搜索
引擎公司爬虫，比如Facebook对Google的屏蔽。但是这种现象仅是商业公司之间的
竞争策略。即便是Facebook，面对自己用户产生的海量数据，依然要依靠搜索技术
来为用户提供满意的服务，区别在于是自己提供搜索技术还是第三方公司来提供。


 ESISmartSpider=travel-finder.com


 广度优先的策略会导致消耗内存非常严重，那就可以用深度优先策略来解决这
 个问题
 点的，
 。因为每次搜的过程，每一层只需要维护一个节点。不过深度优先也是有缺
 因为深度优先的方法是一条路走到黑，
 所以还得继续走其他的路去判断是不是最短路
 无法知道走得这条路是不是最短的，
 ，所以找最优解的时候用广度优先比
 较快。不管是广度优先还是深度优先都可以遍历图中所有的节点，在互联网这张大
 图中，节点是网页，而网页通常是有优先级的，有相对重要的网页，也有不那么重
 要的网页。深度优先策略和广度优先策略只是不断抓取网页并添加到队列尾部，没
 有考虑网页的优先级。


 文件而出现编译错误。本系统使用Python发行包是Anaconda  Anaconda是一个科
学计算包集合，里面提供了很多Python的开发库，包括很多S crapy依赖的库。在
Ubuntu这样的类Unix系统中，可以较方便的通过pip这个Python包管理工具安装
Soapy o

黄恩博.基于布隆过滤器的网页搜索去重方法f Jl.现代计算机:上下旬，2013 (14): 7-10.

f191唐雪峰，宋俊德，宋美娜.基于改进的慢开始算法的网络机器人爬取策略的研究团.新型工
业化，Zo12 }m>.

f $l陈丽.Google搜索引擎架构研究fJl.中国科技纵横，2013 (2): 56-56.

}17} Gupta S, Manchanda P. WebParF: A Web partitioning framework for Parallel Crawlers}J}. arXiv
preprint arXiv:1406.5690, 2014: 15一16


    运用scrapy爬虫框架抓取淘宝分享平台数据。Scrapy是一套基于Twisted的异
步处理框架，纯python实现的爬虫框架。本文实现数据抓取主要解决两个问题:
一是抓取策略，为了最大程度地利用本地带宽，减轻对方服务器的负担，降低爬
虫运行的时空复杂度，设计抓取策略时要深入的分析目标网页面格式，尽量在一
类页面中得到最多的信息，减少网页的访问量。由于淘宝网络的较多页面采用了
动态页面技术，造成爬虫无法提取由页面嵌入的JavaScript代码生成的动态信息。
为了解决这一问题，首先分析目标页面结构找到动态数据源，然后直接抓取数据
源所在网页提取需要数据。二是大规模的测量过程中的环境因素。测量过程中的
网络环境并非稳定的，程序运行过程中可能出现网络中断或机器出现故障等问题。
针对这一问题本文设计了一个安全中间件，主要功能是保证在网络中断时记下当
时的抓取环境，等待网络畅通后继续从中断时的环境开始抓取数据。此外，由于
本文测量的数据规模较大，势必给目标服务器带来较大的负荷，而淘宝网服务器
出于安全和负荷的考虑对客户访问频率设定了限制，所以爬虫的抓取频率要低于
服务器允许的访问频率，否则主机IP将被封掉，一段时间内无法访问目标网页，
抓取过程中断安全中间件会按照网络中断情况来处理，这样会造成时间和空间的
浪费。
    网络爬虫(web crawler)又被称为网络蜘蛛(web spider)，本质上是一段计算
机代码，它从可以按照代码中确定的逻辑下载互联网中特定的网页，是搜索引擎
的一个重要组成部分。爬虫的关键技术包括两部分，一是下载Web页面，这个过
程包含着许多需要考虑的问题，如本地带宽高效利用问题，根据站点特征设计合
理调度策略以减轻对目标服务器的负担等。在一些超大规模的数据测量中，爬虫
系统还需要考虑DNS查询的瓶颈问题。另外，还有一些通用规则需要遵循，例如
robots.txt。另一个关键技术是获取了网页之后的分析，这个过程也是非常复杂的。
    1.爬虫需要考虑的问题。Internet是由成千上万的局域网络连接而成的，其结
构非常复杂且各种HTML页面格式都有，有很多网页格式错误百出，要想得出一
个通用的解析方法几乎是不可能的事。随着AJAX的出现和盛行，如何提取
JavaScript脚本动态生成的网页信息成了所有爬虫无法回避的一大难题，除此之外，
Internet上还不可避免的会出现不同规模的环路，如果不加处理的盲目提取链接的
话，就会陷入无限的循环爬取中，浪费网络带宽和其它资源。
    2.上述问题通用的解决思路。通常为了分析某些特定的网络而编写的。rawler
只是针对性的去爬特定的某个或者某一类网站。可以先对目标网站的结构进行深
入的分析，这样就可以做到有的放矢了。通过分析，选出有意义的urls进行跟踪，
就可以避免跟踪很多不必要的链接或者是陷入无穷尽的Spider   Trap中，如果网站
的结构允许按照路径来提取所需信息的话，我们还可以采用按照信息所在路径把
所有有用信息都爬一遍，省去判断url内容的过程。
    3.爬虫的实质。爬虫从本质上来说很简单，只要需完成下载页面和分析页面两
个功能，现在大部分编程语言都提供了web客户端库，用户可以方便提取Web页
面，解析网页最简单额方法就是直接用正则表达式来提取目标信息，所以要想实
现一个简单的爬虫并不是什么难事，但是要是实现一个高性能的适合大规模数据
抓取的爬虫确实非常难的。
    4.爬虫工作原理。爬虫按照提取数据内容和方式的不同分为，通用爬虫和聚焦
爬虫两种，通用爬虫就是常说的搜索引擎，抓取目标是获得尽量多的网页信息，
而聚焦爬虫只是针对特定的一类或几类网站，抓取目标是网页中特定的信息。图
3.1是两种爬虫工作流程图，通用爬虫是从初始设定的url开始，获取对应网页的
内容，然后提取网页中新的url添加到url队列里，然后再提取url队列出的下一个
url，开始下轮的抓取直到满足停止条件为止。聚焦爬虫的工作流程比通用爬虫复
杂些，提取页面中新的url后聚焦爬虫多了一个提取需求信息的步骤，这便是聚焦
爬虫的特殊所在，爬虫运行的目标就是有目的的提取网页中的信息。此外聚焦爬
虫与通用爬虫还有一个很大的不同，就是提取到的新的url要经过算法筛选后才能
作为下一轮爬取的对象。


# hash算法

#登录ESI

- vpn登录
- 其他学校路径登录 
后期可能失效，需要演示


轮询，线程池 不做了，就用scrapy的